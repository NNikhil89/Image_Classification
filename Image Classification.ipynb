{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c61b92",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d21bf",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568f052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3ce95",
   "metadata": {},
   "source": [
    "### Building CNN with 8 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10438ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map_size 12544\n"
     ]
    }
   ],
   "source": [
    "class CNN8Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN8Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )                \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),            \n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = CNN8Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bfc60",
   "metadata": {},
   "source": [
    "#### Defining Transformation for Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f4135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62021789",
   "metadata": {},
   "source": [
    "#### Defining Transformation for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "282274a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformtest = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfba56",
   "metadata": {},
   "source": [
    "#### Loaders for Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a71f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'F:/Computer Vision Yeshiva/Dog_heart/Train'\n",
    "val_dir = 'F:/Computer Vision Yeshiva/Dog_heart/Valid'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92784f7e",
   "metadata": {},
   "source": [
    "### Training and Evaluating CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3ffa4",
   "metadata": {},
   "source": [
    "#### Training the model using SGD optimiser for the first 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a07c8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.0908\n",
      "Epoch 1/20, Train Loss: 1.0908, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [2/20], Loss: 1.0904\n",
      "Epoch 2/20, Train Loss: 1.0904, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [3/20], Loss: 1.0901\n",
      "Epoch 3/20, Train Loss: 1.0901, Val Loss: 1.0949, Val Acc: 0.4550\n",
      "Epoch [4/20], Loss: 1.0902\n",
      "Epoch 4/20, Train Loss: 1.0902, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [5/20], Loss: 1.0904\n",
      "Epoch 5/20, Train Loss: 1.0904, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [6/20], Loss: 1.0900\n",
      "Epoch 6/20, Train Loss: 1.0900, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [7/20], Loss: 1.0907\n",
      "Epoch 7/20, Train Loss: 1.0907, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [8/20], Loss: 1.0905\n",
      "Epoch 8/20, Train Loss: 1.0905, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [9/20], Loss: 1.0902\n",
      "Epoch 9/20, Train Loss: 1.0902, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [10/20], Loss: 1.0904\n",
      "Epoch 10/20, Train Loss: 1.0904, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [11/20], Loss: 1.0902\n",
      "Epoch 11/20, Train Loss: 1.0902, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [12/20], Loss: 1.0905\n",
      "Epoch 12/20, Train Loss: 1.0905, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [13/20], Loss: 1.0905\n",
      "Epoch 13/20, Train Loss: 1.0905, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [14/20], Loss: 1.0904\n",
      "Epoch 14/20, Train Loss: 1.0904, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [15/20], Loss: 1.0902\n",
      "Epoch 15/20, Train Loss: 1.0902, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [16/20], Loss: 1.0904\n",
      "Epoch 16/20, Train Loss: 1.0904, Val Loss: 1.0948, Val Acc: 0.4550\n",
      "Epoch [17/20], Loss: 1.0904\n",
      "Epoch 17/20, Train Loss: 1.0904, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [18/20], Loss: 1.0903\n",
      "Epoch 18/20, Train Loss: 1.0903, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [19/20], Loss: 1.0905\n",
      "Epoch 19/20, Train Loss: 1.0905, Val Loss: 1.0947, Val Acc: 0.4550\n",
      "Epoch [20/20], Loss: 1.0904\n",
      "Epoch 20/20, Train Loss: 1.0904, Val Loss: 1.0948, Val Acc: 0.4550\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)  # or scheduler.step() for StepLR\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_simplemodel.pth')\n",
    "\n",
    "torch.save(model.state_dict(), 'simple_cnn.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba00f20",
   "metadata": {},
   "source": [
    "#### Training the model using Adam optimiser with learning rate of 0.0001 for the next 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "393129f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40, Train Loss: 1.0276, Val Loss: 1.1779, Val Acc: 0.3800\n",
      "Epoch 23/40, Train Loss: 0.9999, Val Loss: 1.1199, Val Acc: 0.4400\n",
      "Epoch 24/40, Train Loss: 0.9722, Val Loss: 1.1169, Val Acc: 0.4550\n",
      "Epoch 25/40, Train Loss: 0.9598, Val Loss: 1.0487, Val Acc: 0.4250\n",
      "Epoch 26/40, Train Loss: 0.9519, Val Loss: 1.1256, Val Acc: 0.4000\n",
      "Epoch 27/40, Train Loss: 0.9408, Val Loss: 1.0537, Val Acc: 0.4250\n",
      "Epoch 28/40, Train Loss: 0.9254, Val Loss: 0.9887, Val Acc: 0.4400\n",
      "Epoch 29/40, Train Loss: 0.9323, Val Loss: 0.9711, Val Acc: 0.5050\n",
      "Epoch 30/40, Train Loss: 0.9079, Val Loss: 1.0681, Val Acc: 0.4400\n",
      "Epoch 31/40, Train Loss: 0.9029, Val Loss: 0.9807, Val Acc: 0.5000\n",
      "Epoch 32/40, Train Loss: 0.9017, Val Loss: 0.9777, Val Acc: 0.4600\n",
      "Epoch 33/40, Train Loss: 0.8627, Val Loss: 0.8259, Val Acc: 0.5400\n",
      "Epoch 34/40, Train Loss: 0.8279, Val Loss: 0.8618, Val Acc: 0.5650\n",
      "Epoch 35/40, Train Loss: 0.7737, Val Loss: 0.9382, Val Acc: 0.5550\n",
      "Epoch 36/40, Train Loss: 0.7797, Val Loss: 0.8468, Val Acc: 0.5800\n",
      "Epoch 37/40, Train Loss: 0.7717, Val Loss: 0.7736, Val Acc: 0.5600\n",
      "Epoch 38/40, Train Loss: 0.7295, Val Loss: 0.7778, Val Acc: 0.5300\n",
      "Epoch 39/40, Train Loss: 0.7323, Val Loss: 0.7297, Val Acc: 0.6050\n",
      "Epoch 40/40, Train Loss: 0.7114, Val Loss: 0.8120, Val Acc: 0.6150\n",
      "Epoch 41/40, Train Loss: 0.7056, Val Loss: 0.7003, Val Acc: 0.6500\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(21, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:        \n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    scheduler1.step(running_loss/len(train_loader))\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_sgd.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6538272",
   "metadata": {},
   "source": [
    "#### Training the model using Adam optimiser with learning rate of 0.001 for the next 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd20088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/60, Train Loss: 0.8541, Val Loss: 0.7576, Val Acc: 0.6250\n",
      "Epoch 42/60, Train Loss: 0.7626, Val Loss: 0.8492, Val Acc: 0.5750\n",
      "Epoch 43/60, Train Loss: 0.7709, Val Loss: 0.7499, Val Acc: 0.5950\n",
      "Epoch 44/60, Train Loss: 0.7371, Val Loss: 0.9315, Val Acc: 0.5200\n",
      "Epoch 45/60, Train Loss: 0.7225, Val Loss: 0.8200, Val Acc: 0.6300\n",
      "Epoch 46/60, Train Loss: 0.7064, Val Loss: 0.6876, Val Acc: 0.6150\n",
      "Epoch 47/60, Train Loss: 0.6935, Val Loss: 0.5825, Val Acc: 0.6800\n",
      "Epoch 48/60, Train Loss: 0.6788, Val Loss: 0.6373, Val Acc: 0.6900\n",
      "Epoch 49/60, Train Loss: 0.6518, Val Loss: 0.6528, Val Acc: 0.6150\n",
      "Epoch 50/60, Train Loss: 0.6577, Val Loss: 0.6248, Val Acc: 0.6900\n",
      "Epoch 51/60, Train Loss: 0.6586, Val Loss: 0.6481, Val Acc: 0.6900\n",
      "Epoch 52/60, Train Loss: 0.6534, Val Loss: 0.8253, Val Acc: 0.6050\n",
      "Epoch 53/60, Train Loss: 0.6647, Val Loss: 0.6961, Val Acc: 0.6300\n",
      "Epoch 54/60, Train Loss: 0.6380, Val Loss: 0.6465, Val Acc: 0.6650\n",
      "Epoch 55/60, Train Loss: 0.6338, Val Loss: 0.6938, Val Acc: 0.6550\n",
      "Epoch 56/60, Train Loss: 0.5970, Val Loss: 0.6858, Val Acc: 0.6400\n",
      "Epoch 57/60, Train Loss: 0.6270, Val Loss: 0.6615, Val Acc: 0.6300\n",
      "Epoch 58/60, Train Loss: 0.5978, Val Loss: 0.6347, Val Acc: 0.7050\n",
      "Epoch 59/60, Train Loss: 0.6017, Val Loss: 0.6260, Val Acc: 0.6450\n",
      "Epoch 60/60, Train Loss: 0.5982, Val Loss: 0.6826, Val Acc: 0.6550\n",
      "Epoch 61/60, Train Loss: 0.6096, Val Loss: 0.9801, Val Acc: 0.6300\n"
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(40, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:        \n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    scheduler1.step(running_loss/len(train_loader))\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_sgd.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55601b7",
   "metadata": {},
   "source": [
    "#### Training the model using Adam optimiser with learning rate of 0.00001 for the next 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8665fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/80, Train Loss: 0.5950, Val Loss: 0.8138, Val Acc: 0.6150\n",
      "Epoch 62/80, Train Loss: 0.5634, Val Loss: 0.7245, Val Acc: 0.6500\n",
      "Epoch 63/80, Train Loss: 0.5334, Val Loss: 0.6528, Val Acc: 0.6600\n",
      "Epoch 64/80, Train Loss: 0.5633, Val Loss: 0.6853, Val Acc: 0.6600\n",
      "Epoch 65/80, Train Loss: 0.5517, Val Loss: 0.6716, Val Acc: 0.6500\n",
      "Epoch 66/80, Train Loss: 0.5425, Val Loss: 0.7265, Val Acc: 0.6600\n",
      "Epoch 67/80, Train Loss: 0.5391, Val Loss: 0.7152, Val Acc: 0.6550\n",
      "Epoch 68/80, Train Loss: 0.5233, Val Loss: 0.6703, Val Acc: 0.6750\n",
      "Epoch 69/80, Train Loss: 0.5117, Val Loss: 0.6490, Val Acc: 0.7000\n",
      "Epoch 70/80, Train Loss: 0.5419, Val Loss: 0.6674, Val Acc: 0.6750\n",
      "Epoch 71/80, Train Loss: 0.5298, Val Loss: 0.6250, Val Acc: 0.6900\n",
      "Epoch 72/80, Train Loss: 0.5232, Val Loss: 0.6940, Val Acc: 0.6600\n",
      "Epoch 73/80, Train Loss: 0.5174, Val Loss: 0.6375, Val Acc: 0.6650\n",
      "Epoch 74/80, Train Loss: 0.5114, Val Loss: 0.6950, Val Acc: 0.6800\n",
      "Epoch 75/80, Train Loss: 0.5295, Val Loss: 0.6834, Val Acc: 0.6700\n",
      "Epoch 76/80, Train Loss: 0.5238, Val Loss: 0.6815, Val Acc: 0.6550\n",
      "Epoch 77/80, Train Loss: 0.5184, Val Loss: 0.6361, Val Acc: 0.6850\n",
      "Epoch 78/80, Train Loss: 0.5144, Val Loss: 0.7133, Val Acc: 0.6650\n",
      "Epoch 79/80, Train Loss: 0.5056, Val Loss: 0.7015, Val Acc: 0.6500\n",
      "Epoch 80/80, Train Loss: 0.5137, Val Loss: 0.6696, Val Acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-4)\n",
    "scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "\n",
    "num_epochs = 80\n",
    "\n",
    "for epoch in range(60, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:        \n",
    "        optimizer2.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    scheduler1.step(running_loss/len(train_loader))\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_sgd.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32870bc",
   "metadata": {},
   "source": [
    "##### At the end of 80th epoch we got training loss of 0.5 and Validation Accuracy of 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37debb",
   "metadata": {},
   "source": [
    "### Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27d5a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'CNNetwork_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf6af2",
   "metadata": {},
   "source": [
    "### Loader for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b47f58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_filepaths = []\n",
    "        self.image_filenames = []\n",
    "        \n",
    "        for label in os.listdir(root_dir):            \n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            #print(\"label dir\", label_dir)\n",
    "            self.image_filepaths.append(label_dir)\n",
    "            self.image_filenames.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_filepaths[idx]\n",
    "        img_name = self.image_filenames[idx]\n",
    "        #image = cv2.imread(img_path)\n",
    "        image = Image.open(img_path).convert(\"RGB\")        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path, img_name\n",
    "\n",
    "# Create a test data loader\n",
    "test_batch_size = 8\n",
    "test_dataset = CustomImageDataset(root_dir='F:/Computer Vision Yeshiva/Test', transform=transformtest)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88282b95",
   "metadata": {},
   "source": [
    "### Making Predictions on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aca0ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_file_names = []\n",
    "all_file_paths = []\n",
    "\n",
    "model.load_state_dict(torch.load('CNNetwork_model_weights.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient calculations for inference\n",
    "    for inputs, file_paths,file_names in test_loader:\n",
    "        #inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        #print(\"p is \", predicted)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        #all_labels.extend(labels.cpu().numpy())\n",
    "        all_file_names.extend(file_names)\n",
    "        all_file_paths.extend(file_paths)\n",
    "        \n",
    "# Create a dictionary with file names and predictions\n",
    "data = {'File Name': all_file_names, 'Predicted Label': all_predictions}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = 'F:/Computer Vision Yeshiva/CNNDogHeartPrediction.csv'\n",
    "df.to_csv(output_csv_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0f892",
   "metadata": {},
   "source": [
    "Accuracy of the CNN8Net model on Dog Heart Test Dataset is 71%."
   ]
  },
  {
   "attachments": {
    "ComparisonResults.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ed2f6450",
   "metadata": {},
   "source": [
    "![ComparisonResults.png](attachment:ComparisonResults.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d4d8b",
   "metadata": {},
   "source": [
    "### ResearchGate paper link for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa977f0",
   "metadata": {},
   "source": [
    "https://www.researchgate.net/publication/382111878_Implementation_of_Convolutional_Neural_Network_for_Image_Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fb4a4",
   "metadata": {},
   "source": [
    "### GitHub Weight link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423d3ac",
   "metadata": {},
   "source": [
    "https://github.com/NNikhil89/CNNPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24ff43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
